# Event Log Demo - Failure Scenario Drills

This guide walks through various failure scenarios to demonstrate data pipeline resilience, queue buildup, and recovery behavior.

## Prerequisites

- All services running: `docker compose up -d`
- Producer streaming logs: Run `./scripts/demo/resume-producer.sh`
- Monitor dashboard open: http://localhost:5000/monitor

## Monitoring Metrics

The `/monitor` endpoint shows real-time metrics:
- `producer_count`: Total logs generated by producer
- `kafka_count`: Total logs in Kafka topic (end offset)
- `cassandra_count`: Total logs persisted in Cassandra
- `is_streaming`: Whether producer is actively generating logs

Refresh every 10 seconds to see changes.

---

## Drill 1: Kafka Broker Failure

**Objective**: Demonstrate producer-side buffering when Kafka is unavailable.

### Steps:
1. Baseline: Note current metrics values
2. Stop Kafka: `./scripts/demo/stop-kafka.sh`
3. Observe for 30 seconds:
   - `producer_count` continues increasing (in-memory queue)
   - `kafka_count` remains static (broker down)
   - `cassandra_count` remains static (no new data from Kafka)
4. Restart Kafka: `./scripts/demo/restart-kafka.sh`
5. Observe recovery:
   - `kafka_count` rapidly increases as queued messages flush
   - `cassandra_count` resumes increasing via Flink processing

### Expected Behavior:
- **Buffering**: Producer queues messages in memory when Kafka is down
- **No Data Loss**: All buffered messages reach Kafka after restart
- **Recovery Time**: ~5-10 seconds for full flush

---

## Drill 2: Flink Job Manager Failure

**Objective**: Demonstrate queue buildup in Kafka when processing stops.

### Steps:
1. Baseline: Ensure producer is streaming and all services healthy
2. Stop Flink: `./scripts/demo/stop-flink.sh`
3. Observe for 60 seconds:
   - `producer_count` continues increasing
   - `kafka_count` continues increasing (producer still writing)
   - `cassandra_count` remains static (Flink not processing)
   - **Lag grows**: `kafka_count - cassandra_count` increases
4. Restart Flink: `./scripts/demo/restart-flink.sh`
5. Observe recovery:
   - `cassandra_count` increases rapidly (backlog processing)
   - Lag decreases until `cassandra_count` ≈ `kafka_count`

### Expected Behavior:
- **Queue Buildup**: Kafka acts as durable buffer during Flink outage
- **Backpressure**: No backpressure on producer (Kafka absorbs load)
- **Recovery**: Flink processes backlog at maximum throughput

---

## Drill 3: Cassandra Database Failure

**Objective**: Demonstrate data durability in Kafka when sink is unavailable.

### Steps:
1. Baseline: All services running, producer streaming
2. Stop Cassandra: `./scripts/demo/stop-cassandra.sh`
3. Observe for 45 seconds:
   - `producer_count` increases (normal operation)
   - `kafka_count` increases (normal operation)
   - `cassandra_count` remains static (database down)
   - Flink continues running but cannot write to sink
4. Restart Cassandra: `./scripts/demo/restart-cassandra.sh`
5. Observe recovery:
   - `cassandra_count` resumes increasing
   - Flink flushes buffered writes

### Expected Behavior:
- **Data Preservation**: All data remains in Kafka during outage
- **No Data Loss**: Flink retries writes after Cassandra recovery
- **Resilience**: Upstream services unaffected by sink failure

---

## Drill 4: Complete Pipeline Stop and Resume

**Objective**: Demonstrate clean shutdown and restart of entire pipeline.

### Steps:
1. Baseline: Producer streaming, all metrics growing
2. Pause producer: `./scripts/demo/pause-producer.sh`
3. Observe: All metrics static (no new data entering pipeline)
4. Stop Flink: `./scripts/demo/stop-flink.sh`
5. Stop Kafka: `./scripts/demo/stop-kafka.sh`
6. All components stopped - pipeline fully idle
7. Resume in reverse order:
   - `./scripts/demo/restart-kafka.sh`
   - `./scripts/demo/restart-flink.sh`
   - `./scripts/demo/resume-producer.sh`
8. Observe: All metrics resume growing

### Expected Behavior:
- **Clean Shutdown**: No data loss during graceful stop
- **State Preservation**: Offsets and counts preserved
- **Resume**: Pipeline resumes from last committed positions

---

## Drill 5: Cascading Failure Recovery

**Objective**: Demonstrate recovery from multiple simultaneous failures.

### Steps:
1. Baseline: Producer streaming at 10 logs/sec
2. Trigger cascading failure:
   ```bash
   ./scripts/demo/stop-flink.sh
   sleep 5
   ./scripts/demo/stop-kafka.sh
   ```
3. Observe: 
   - `producer_count` increases (buffering in memory)
   - `kafka_count` static (broker down)
   - `cassandra_count` static (Flink down)
4. Recover services in order:
   ```bash
   ./scripts/demo/restart-kafka.sh
   sleep 10  # Allow Kafka to stabilize
   ./scripts/demo/restart-flink.sh
   ```
5. Observe recovery:
   - Producer flushes to Kafka
   - Flink processes backlog
   - Metrics converge

### Expected Behavior:
- **Buffering Chain**: Each component buffers for downstream
- **No Data Loss**: All data preserved through multi-component failure
- **Recovery Order Matters**: Start from sink → source for optimal recovery

---

## Disk Utilization Monitoring

### Kafka Disk Usage
```bash
docker exec event-log-demo-kafka-1 du -sh /var/lib/kafka/data
```

### Cassandra Disk Usage
```bash
docker exec event-log-demo-cassandra-1 du -sh /var/lib/cassandra/data
```

### Expected Growth Rates:
- **Kafka**: ~10MB/min at 10 logs/sec, 1KB each
- **Cassandra**: ~8MB/min (after compression)
- **Kafka Cleanup**: Topic retention will clean old data (default: 7 days)

---

## Troubleshooting

### Producer Not Streaming
Check status: `curl http://localhost:5000/monitor`
If `is_streaming: false`, run: `./scripts/demo/resume-producer.sh`

### Service Won't Start
Check Docker logs: `docker compose logs <service>`
Verify dependencies: `docker compose ps`

### Metrics Not Updating
Check producer logs: `docker compose logs producer`
Verify scheduler is running: Look for "Metrics refreshed" messages

### API Control Failing
Verify Docker socket mounted: `docker exec event-log-demo-producer-1 ls -la /var/run/docker.sock`
Check compose file exists: `docker exec event-log-demo-producer-1 ls -la /docker-compose.yml`

---

## Cleanup

Stop all services:
```bash
docker compose down
```

Remove volumes (resets all data):
```bash
docker compose down -v
```
